{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a6c17c",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* Intro: process & technical stack\n",
    "   \n",
    "\n",
    "\n",
    "* 1. Scraping scripts\n",
    "    * 1.1 Scraping with Python and aws (boto3) \n",
    "        * 1.1.1 Scraping a list of listing ids for a set area and dates range\n",
    "        * 1.1.2 Script for scraping reviews & calendar Data\n",
    "    * 1.2 Docker & AWS\n",
    "\n",
    "\n",
    "\n",
    "* 2. Processing and storing in database\n",
    "    * 2.1 Working locally\n",
    "    * 2.2 Parsing and processing JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf3557",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2c577f",
   "metadata": {},
   "source": [
    "The first step of this project is to collect and process data from Airbnb. \n",
    "The first part of this notebook presents the scripts used to scrap the data and that are dockerized and run on AWS EC2. \n",
    "The second part describes the scripts used to process and store the data in MySQL.\n",
    "\n",
    "Data was collected in november 2021. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dcf3fa",
   "metadata": {},
   "source": [
    "## Use of the Cloud AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672809b4",
   "metadata": {},
   "source": [
    "![{1280x720}](scraping.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80e095",
   "metadata": {},
   "source": [
    "### Why EC2 instead of ECS or Lambda ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c38f6",
   "metadata": {},
   "source": [
    "**Docker-compose and ECS and Lambda:** \n",
    "\n",
    "Ideally I would have used ECS for this project as would have allowed to spawn servers on demand and to speed up the process. \n",
    "Unfortunately I needed to operate behind a VPN to avoid being banned by Airbnb (even though the scraping rate was really slow with request every 15 seconds or more). I hence needed to use a dockerized version of the VPN but sadly docker-compose was not working on ECS at the time. \n",
    "\n",
    "I also found it more difficult to work behind VPN from Lambda but this solution could have cut the use of queues. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bca3fa",
   "metadata": {},
   "source": [
    "# 1. Scraping scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd43fc",
   "metadata": {},
   "source": [
    "## 1.1 Scraping with python and boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc9375",
   "metadata": {},
   "source": [
    "### 1.1.1 Scraping a list of listing ids for a set area and dates range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b6b9f",
   "metadata": {},
   "source": [
    "The Following script is used to collect a list of listing IDs for a specified geographical area.\n",
    "\n",
    "The script uses the following steps:\n",
    "1. Choose a target area and get its borders in terms of longitude and latitude coordinates.\n",
    "2. Scan the area with smaller size increments. \n",
    "3. For each sub-sized increment collect a list of listings\n",
    "4. Feed those listings to downstream queues for details scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------IMPORTS---------------------------------------------------------------------------\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import boto3\n",
    "import logging\n",
    "from math import ceil\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#  ---------------PARAMETRAGE----------------------------------------------------------------------------\n",
    "\n",
    "# QUEUE LISTENING FREQUENCY\n",
    "freq = 6\n",
    "\n",
    "# SESSION A MODIFIER POUR BASCULER DU COMTPE AMAURY(DEV) A HELL(PROD)\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id='access_key',\n",
    "    aws_secret_access_key='secret_access_key'\n",
    ")\n",
    "\n",
    "# URLS DES QUEUES\n",
    "QUEUES_URLS = {\"starting\": \"URL/starting-queue.fifo\",\n",
    "               \"IDs\": \"URL/ids-scraping.fifo\",\n",
    "               \"ReviewsCalendar\": \"URL/RC-Q.fifo\",\n",
    "               \"Prices\": \"URL/PDP-Q.fifo\",\n",
    "               }\n",
    "\n",
    "#  S3 BUCKETS USED\n",
    "buckets = {\"listings\": \"abnb-listings\",\n",
    "           \"prices\": \"abnb-prices\",\n",
    "           \"reviews\": \"abnb-reviews\",\n",
    "           \"calendar\": \"abnb-calendar\"}\n",
    "\n",
    "# COORDONNEES ET LISTE DES VILLES A SCRAPER\n",
    "search_areas = {\n",
    "          \"Reunion\": [-20.8714, 55.8403, -21.3893, 55.2086, 10],\n",
    "          }\n",
    "\n",
    "# REGION, NORMALEMENT NE BOUGE PAS \n",
    "AWS_REGION = 'eu-west-3'\n",
    "\n",
    "# SETTING SQS CLIENT, RAF\n",
    "sqs_client = session.client(\"sqs\", region_name=AWS_REGION)\n",
    "\n",
    "# SETTING LOGGING\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "\n",
    "\n",
    "# ---------------------SCRIPTS---------------------------------------------------------------------------\n",
    "\n",
    "def handler(freq, buckets, QUEUES_URLS):\n",
    "    FROM_QUEUE_URL = QUEUES_URLS['starting']\n",
    "    on = True\n",
    "    while on:\n",
    "        time.sleep(freq)\n",
    "        messages = receive_queue_message(FROM_QUEUE_URL)\n",
    "        print(messages)\n",
    "        try:\n",
    "            test = messages['Messages']\n",
    "        except Exception:\n",
    "            print(Exception)\n",
    "            logger.info(\"No message in the queue\")\n",
    "        else:\n",
    "            try:\n",
    "                for msg in messages['Messages']:\n",
    "                    # RETRIEVING PARAMETERS FROM MESSAGE\n",
    "                    msg_body = msg['Body']\n",
    "                    body_json = json.loads(msg_body)\n",
    "                    city = body_json['city']\n",
    "                    dates_list = eval(body_json['dates_list'])\n",
    "                    print(msg_body, city, dates_list)\n",
    "\n",
    "                    # LAUNCHING SCRAPING SCRIPT\n",
    "                    main_script(city, dates_list, buckets['main'], QUEUES_URLS)\n",
    "\n",
    "                    # DELETING MESSAGE\n",
    "                    receipt_handle = msg['ReceiptHandle']\n",
    "                    logger.info(f'The message body: {msg_body}')\n",
    "                    logger.info('Deleting message from the queue...')\n",
    "                    delete_queue_message(FROM_QUEUE_URL, receipt_handle)\n",
    "                    logger.info(f'Received and deleted message(s) from {FROM_QUEUE_URL}.')\n",
    "            except Exception:\n",
    "                print(Exception)\n",
    "\n",
    "    return {\n",
    "        'message': f'scrape reviews code executed correctly for area {city}'\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_iter_lengths():\n",
    "    \"\"\"\n",
    "    This function computes the width and heights increments used to scan the target area.\n",
    "    Here the increments correspond to a zoom 13 on OpenStreetMap\n",
    "    \"\"\"\n",
    "    ne_lat = 43.6386\n",
    "    ne_lng = 1.5106\n",
    "    sw_lat = 43.5596\n",
    "    sw_lng = 1.3510\n",
    "    zoom = 13\n",
    "    iter_width = ne_lat - sw_lat\n",
    "    iter_height = ne_lng - sw_lng\n",
    "\n",
    "    return iter_width, iter_height\n",
    "\n",
    "\n",
    "def compute_coords_settings(search_areas, city):\n",
    "    \"\"\"\n",
    "    This function aims at preparing the paremeters used to scan the target area in the main script.\n",
    "    The target area is defined as a rectangle and is scanned from bottom left corner to top right corner \n",
    "    by smaller areas of size iter_width * iter_height\n",
    "    \"\"\"\n",
    "    iter_width, iter_height = compute_iter_lengths()\n",
    "\n",
    "    max_ne_lat = search_areas[city][0]\n",
    "    max_ne_lng = search_areas[city][1]\n",
    "    min_sw_lat = search_areas[city][2]\n",
    "    min_sw_lng = search_areas[city][3]\n",
    "\n",
    "    total_width = max_ne_lat - min_sw_lat\n",
    "    total_height = max_ne_lng - min_sw_lng\n",
    "\n",
    "    width_ratio = total_width / iter_width\n",
    "    height_ratio = total_height / iter_height\n",
    "\n",
    "    num_of_width_iter = int(ceil(width_ratio))\n",
    "    num_of_height_iter = int(ceil(height_ratio))\n",
    "\n",
    "    return min_sw_lat, min_sw_lng, iter_width, iter_height, num_of_width_iter, num_of_height_iter\n",
    "\n",
    "\n",
    "def main_script(city, dates_list, bucket_name, QUEUES_URLS):\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we iterate our way over the target area. For each iteration area we scrap the corresponding listing ids \n",
    "    that are then passed along to waiting queues in order to scrape detail information for each id. \n",
    "    \"\"\"\n",
    "    \n",
    "    # create the parameters used to to scan the target area\n",
    "    min_sw_lat, min_sw_lng, iter_width, iter_height, num_of_width_iter, num_of_height_iter = compute_coords_settings(\n",
    "        cities, city)\n",
    "\n",
    "    # iterate over the sub areas     \n",
    "    count = 0\n",
    "    # listings_ids is a list used to stored all unique ids found in search results     \n",
    "    listing_ids = []\n",
    "    # We check several arrival and departure dates as some listings may already be booked, \n",
    "    # or require longer stays and hence not appear in the search results     \n",
    "    for dates in dates_list:\n",
    "        for i in range(num_of_width_iter):\n",
    "            for j in range(num_of_height_iter):\n",
    "                sw_lat = min_sw_lat + i * iter_width\n",
    "                ne_lat = min_sw_lat + (i + 1) * iter_width\n",
    "                sw_lng = min_sw_lng + j * iter_height\n",
    "                ne_lng = min_sw_lng + (j + 1) * iter_height\n",
    "                count = count + 1\n",
    "                # current_ids is a list used to stored all unique ids found in the current scanned area and date\n",
    "                current_ids = []\n",
    "                continue_searching = True\n",
    "                for k in range(15):\n",
    "                    if continue_searching == True:\n",
    "                        \n",
    "                        # A random time constraint is set to very slowly ask data from Abnb \n",
    "                        # in order not to overload the server and get banned                         \n",
    "                        time.sleep(random.uniform(15, 20))\n",
    "                        \n",
    "                        # there is amaximum of 20 listings per search page on Abnb (max 15 result pages).\n",
    "                        # We use this offset variable to navigate through the result pages.                          \n",
    "                        offset = k * 20\n",
    "                        \n",
    "                        try:\n",
    "                            check_in = dates[0]\n",
    "                            check_out = dates[1]\n",
    "                            # The 'API' request is called to retrieve data back from Abnb's servers\n",
    "                            response = scrape_map_area(check_in, check_out, sw_lat, ne_lat, sw_lng, ne_lng, offset)\n",
    "                            response_text = response.text\n",
    "                            print(response_text)\n",
    "                            data = json.loads(response_text)\n",
    "                            # Let's check if we got any data and in the right format                             \n",
    "                            try:\n",
    "                                items = data[\"data\"][\"dora\"][\"exploreV3\"][\"sections\"][0]['items']\n",
    "                            except:\n",
    "                                items = data['data']['dora'][\"exploreV3\"]['searchFooter']['sections'][0]['items']\n",
    "\n",
    "                           # Here we do a first parse of the JSON to get back the listing IDs.\n",
    "                           # If we already encountered the id then we went through all result pages \n",
    "                           # and we can go on to scan the next sub_area / date\n",
    "                           # Else, we get back new data and we store it in a JSON as a security \n",
    "                           # in case detail scraping scripts fail   \n",
    "                            for i, item in enumerate(items):\n",
    "                                id = item['listing']['id']\n",
    "                                if id in current_ids:\n",
    "                                    continue_searching = False\n",
    "                                    break\n",
    "                                else:\n",
    "                                    current_ids.append(id)\n",
    "                            if continue_searching:\n",
    "                                file_path = save_dataset_to_json(data,\n",
    "                                                                 bucket_name,\n",
    "                                                                 city,\n",
    "                                                                 check_in,\n",
    "                                                                 check_out,\n",
    "                                                                 count,\n",
    "                                                                 k)\n",
    "                                print(f\"JSON created at{file_path}\")   \n",
    "\n",
    "                            # If we already encountered we don't send new requests to the server  \n",
    "                            for i, item in enumerate(items):\n",
    "                                id = item['listing']['id']\n",
    "                                print(\"id:\", id, \"i\", i)\n",
    "                                if id in listing_ids:\n",
    "                                    pass\n",
    "                                # If we encounter a new ID, we forward the relevant information to launch \n",
    "                                # the detail scraping scripts\n",
    "                                else:\n",
    "                                    listing_ids.append(id)\n",
    "                                    capacity = item['listing']['personCapacity']\n",
    "                                    feed_forward_queue(\"RC\", \"RC\", city, id, check_in, check_out, \"\", \"\", QUEUES_URLS)\n",
    "                                    feed_forward_queue(\"concealed\", \"concealed\", city, id, check_in, check_out, \"\",\n",
    "                                                       capacity,\n",
    "                                                       QUEUES_URLS)\n",
    "\n",
    "                        except Exception:\n",
    "                            print(Exception)\n",
    "                    else:\n",
    "                        break\n",
    "    return\n",
    "\n",
    "\n",
    "def scrape_map_area(check_in_date, check_out_date, sw_lat, ne_lat, sw_lng, ne_lng, offset):\n",
    "     \n",
    "    \"\"\"\n",
    "    script to request data from Abnb's server. Built with postman and parametrize for iterations variables. \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    url = f\"https://www.airbnb.fr/api/v3/ExploreSearch?operationName=ExploreSearch&locale=fr&currency=EUR&variables=%7B%22request%22%3A%7B%22metadataOnly%22%3Afalse%2C%22version%22%3A%221.8.2%22%2C%22itemsPerGrid%22%3A20%2C%22tabId%22%3A%22home_tab%22%2C%22refinementPaths%22%3A%5B%22%2Fhomes%22%5D%2C%22flexibleTripDates%22%3A%5B%22november%22%2C%22october%22%5D%2C%22flexibleTripLengths%22%3A%5B%22weekend_trip%22%5D%2C%22datePickerType%22%3A%22calendar%22%2C%22checkin%22%3A%22{check_in_date}%22%2C%22checkout%22%3A%22{check_out_date}%22%2C%22adults%22%3A1%2C%22source%22%3A%22structured_search_input_header%22%2C%22searchType%22%3A%22user_map_move%22%2C%22neLat%22%3A%22{ne_lat}%22%2C%22neLng%22%3A%22{ne_lng}%22%2C%22swLat%22%3A%22{sw_lat}%22%2C%22swLng%22%3A%22{sw_lng}%22%2C%22searchByMap%22%3Atrue%2C%22placeId%22%3A%22ChIJMySoO04FU4cRniPNrMWMPKg%22%2C%22federatedSearchSessionId%22%3A%22740ec27a-6b30-4b7e-97e2-cd74bd89134f%22%2C%22itemsOffset%22%3A{offset}%2C%22sectionOffset%22%3A3%2C%22query%22%3A%22Le%20Mans%22%2C%22cdnCacheSafe%22%3Afalse%2C%22treatmentFlags%22%3A%5B%22flex_destinations_june_2021_launch_web_treatment%22%2C%22flexible_dates_options_extend_one_three_seven_days%22%2C%22super_date_flexibility%22%2C%22search_input_placeholder_phrases%22%5D%2C%22screenSize%22%3A%22large%22%7D%7D&extensions=%7B%22persistedQuery%22%3A%7B%22version%22%3A1%2C%22sha256Hash%22%3A%22d81d0c85fc1e815c9d5718eef4398f8f3b8fb1e2c5e325989f1313fa6c3abfec%22%7D%7D&_cb=0yijg6h11pe0420yg1wm11ocv8sj\"\n",
    "\n",
    "    payload = {}\n",
    "    headers = {\n",
    "        'authority': 'www.airbnb.fr',\n",
    "        'sec-ch-ua': '\"Chromium\";v=\"94\", \"Google Chrome\";v=\"94\", \";Not A Brand\";v=\"99\"',\n",
    "        'x-csrf-token': 'V4$.airbnb.fr$XMi705x7GDI$NxAv8KN4mVwsa9gU8R9nJrugydYqmn_oL935U8kU=',\n",
    "        'x-airbnb-api-key': 'd306zoyjsyarp7ifhu67rjxn52tv0t20',\n",
    "        'x-niobe-short-circuited': 'true',\n",
    "        'dpr': '1.25',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "        'device-memory': '8',\n",
    "        'x-airbnb-graphql-platform-client': 'minimalist-niobe',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'x-csrf-without-token': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36',\n",
    "        'viewport-width': '955',\n",
    "        'content-type': 'application/json',\n",
    "        'x-airbnb-supports-airlock-v2': 'true',\n",
    "        'ect': '4g',\n",
    "        'x-airbnb-graphql-platform': 'web',\n",
    "        'accept': '*/*',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-mode': 'cors',\n",
    "        'sec-fetch-dest': 'empty',\n",
    "        'referer': f'https://www.airbnb.fr/Reunion/homes?tab_id=home_tab&refinement_paths%5B%5D=%2Fhomes&flexible_trip_dates%5B%5D=november&flexible_trip_dates%5B%5D=october&flexible_trip_lengths%5B%5D=weekend_trip&date_picker_type=calendar&checkin={check_in_date}&checkout={check_out_date}&adults=1&source=structured_search_input_header&search_type=user_map_move&ne_lat={ne_lat}&ne_lng={ne_lng}&sw_lat={sw_lat}&sw_lng={sw_lng}&zoom=15&search_by_map=true&place_id=ChIJMySoO04FU4cRniPNrMWMPKg&federated_search_session_id=740ec27a-6b30-4b7e-97e2-cd74bd89134f&pagination_search=true&items_offset={offset}&section_offset=3',\n",
    "        'accept-language': 'fr-FR,fr;q=0.9',\n",
    "        'cookie': 'Session_Cookie'  }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def send_queue_message(queue_url, msg_attributes, msg_body):\n",
    "    \"\"\"\n",
    "    Sends a message to the specified queue.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = sqs_client.send_message(QueueUrl=queue_url,\n",
    "                                           MessageAttributes=msg_attributes,\n",
    "                                           MessageBody=msg_body,\n",
    "                                           MessageGroupId=str(random.randint(0, 1000)))\n",
    "    except ClientError:\n",
    "        logger.exception(f'Could not send meessage to the - {queue_url}.')\n",
    "        raise\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "\n",
    "def save_dataset_to_json(dataset, bucketname, city, check_in, check_out, count, k):\n",
    "    \"\"\"\n",
    "    function that saves a dataset to a JSON file in the targeted AWS Bucket\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = f'RAW_JSON/main/{city}_{check_in}_{check_out}_iter_{count}_page{k + 1}.json'\n",
    "\n",
    "    s3 = session.resource('s3', region_name=AWS_REGION)\n",
    "\n",
    "    s3object = s3.Object(bucketname, file_name)\n",
    "\n",
    "    s3object.put(\n",
    "        Body=(bytes(json.dumps(dataset).encode('UTF-8')))\n",
    "    )\n",
    "\n",
    "    return file_name\n",
    "\n",
    "\n",
    "def receive_queue_message(queue_url):\n",
    "    \"\"\"\n",
    "    Retrieves one or more messages (up to 10), from the specified queue.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = sqs_client.receive_message(QueueUrl=queue_url)\n",
    "    except ClientError:\n",
    "        logger.exception(\n",
    "            f'Could not receive the message from the - {queue_url}.')\n",
    "        raise\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "\n",
    "def delete_queue_message(queue_url, receipt_handle):\n",
    "    \"\"\"\n",
    "    Deletes the specified message from the specified queue.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = sqs_client.delete_message(QueueUrl=queue_url,\n",
    "                                             ReceiptHandle=receipt_handle)\n",
    "    except ClientError:\n",
    "        logger.exception(\n",
    "            f'Could not delete the meessage from the - {queue_url}.')\n",
    "        raise\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "def feed_forward_queue(queue, cat, city, listing_id, check_in, check_out, file_path, capacity, QUEUES_URLS):\n",
    "    \n",
    "    \"\"\"\n",
    "    Feeds downstream queues with the parameters needed to scrape detailed data about unique listing IDs\n",
    "    \"\"\"\n",
    "    MSG_ATTRIBUTES = {\n",
    "        'Title': {\n",
    "            'DataType': 'String',\n",
    "            'StringValue': 'my_scraping_project'\n",
    "        },\n",
    "        'Author': {\n",
    "            'DataType': 'String',\n",
    "            'StringValue': 'scrapingmain'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    MSG_BODY = '{' + f'\"cat\":\"{cat}\", \"city\":\"{city}\",\"capacity\":\"{capacity}\",\"file_path\":\"{file_path}\",\"listing_id\":\"{listing_id}\",\"check_in\":\"{check_in}\",' \\\n",
    "                     f'\"check_out\":\"{check_out}\"' + \"}\"\n",
    "\n",
    "    msg = send_queue_message(QUEUES_URLS[queue], MSG_ATTRIBUTES, MSG_BODY)\n",
    "    json_msg = json.dumps(msg, indent=4)\n",
    "\n",
    "    logger.info(f'''\n",
    "        Message sent to the queue {QUEUES_URLS[cat]}.\n",
    "        Message attributes: \\n{json_msg}''')\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "#  ----------------MAIN----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    handler(freq, buckets, QUEUES_URLS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ecaed",
   "metadata": {},
   "source": [
    "### What I could have done better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cec6d9",
   "metadata": {},
   "source": [
    "I chose at first to keep all functions in one script as it was easier to dockerize. But I probably should have separated functions used to interact with AWS products to make the code more readable and reusable. \n",
    "Also I could have parametrized area variables in a more efficient way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe164a6",
   "metadata": {},
   "source": [
    "### 1.1.2 Script for scraping reviews & calendar Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf9a75a",
   "metadata": {},
   "source": [
    "Once we get a list of listing IDs on the target area and dates, we scrape detailed information for each individual listing. Below is the script used to scrape Reviews and Calendar Data. \n",
    "1. we listen for new listing ids in the Reviews & Calendar Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf1e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose# ---------------------IMPORTS---------------------------------------------------------------------------\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import boto3\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#  ---------------PARAMETRAGE A VERIFIER-------------------------------------------------------------------\n",
    "# QUEUE LISTENING FREQUENCY\n",
    "freq=60\n",
    "\n",
    "# SESSION A MODIFIER POUR BASCULER DU COMTPE AMAURY(DEV) A HELL(PROD)\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id='access_key',\n",
    "    aws_secret_access_key='secret_access_key'\n",
    ")\n",
    "\n",
    "# URLS DES QUEUES\n",
    "QUEUES_URLS = {\"starting\": \"URL/starting-queue.fifo\",\n",
    "               \"IDs\": \"URL/ids-scraping.fifo\",\n",
    "               \"ReviewsCalendar\": \"URL/RC-Q.fifo\",\n",
    "               \"Prices\": \"URL/PDP-Q.fifo\",\n",
    "               }\n",
    "\n",
    "#  S3 BUCKETS USED\n",
    "buckets = {\"listings\": \"abnb-listings\",\n",
    "           \"prices\": \"abnb-prices\",\n",
    "           \"reviews\": \"abnb-reviews\",\n",
    "           \"calendar\": \"abnb-calendar\"}\n",
    "\n",
    "\n",
    "#  ----------------PARAMETRAGE FIXE, RIEN A MODIFIER SI TOUT VA BIEN ----------------------------------------------------------------------\n",
    "# REGION, NORMALEMENT NE BOUGE PAS \n",
    "AWS_REGION = 'eu-west-3'\n",
    "\n",
    "# SETTING SQS CLIENT, RAF\n",
    "sqs_client = session.client(\"sqs\", region_name=AWS_REGION)\n",
    "\n",
    "# SETTING LOGGING\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "\n",
    "\n",
    "# ---------------------SCRIPTS, RIEN A MODIFIER SI TOUT VA BIEN------------------------------------------\n",
    "def handler(freq,buckets,QUEUES_URLS):\n",
    "    FROM_QUEUE_URL=QUEUES_URLS[\"RC\"]\n",
    "    on = True\n",
    "    while on:\n",
    "        time.sleep(freq)\n",
    "        messages = receive_queue_message(FROM_QUEUE_URL)\n",
    "        print(messages)\n",
    "        try:\n",
    "            test = messages['Messages']\n",
    "        except Exception:\n",
    "            print(Exception)\n",
    "            logger.info(\"No message in the queue\")\n",
    "        else:\n",
    "            try:\n",
    "                for msg in messages['Messages']:\n",
    "                    # RETRIEVING PARAMETERS FROM MESSAGE\n",
    "                    msg_body = msg['Body']\n",
    "                    body_json = json.loads(msg_body)\n",
    "                    city = body_json['city']\n",
    "                    check_in = body_json['check_in']\n",
    "                    check_out = body_json['check_out']\n",
    "                    listing_id = body_json['listing_id']\n",
    "                    print(msg_body,city,check_in,check_out,listing_id)\n",
    "\n",
    "                    # LAUNCHING REVIEWS SCRAPING SCRIPT\n",
    "                    reviews_response = scrape_reviews(check_in, check_out, listing_id)\n",
    "                    reviews_response_text = reviews_response.text\n",
    "                    print(reviews_response_text)\n",
    "                    reviews_file_path = save_dataset_to_json(reviews_response_text, buckets['reviews'], city, listing_id)\n",
    "                    print('REVIEWS JSON CREATED')\n",
    "                    feed_forward_queue(\"parsing\",\"reviews\",city,listing_id,check_in,check_out,reviews_file_path,QUEUES_URLS)\n",
    "\n",
    "                    # LAUNCHING CALENDAR SCRAPING SCRIPT\n",
    "                    calendar_response=scrape_calendar(check_in,check_out, listing_id, count=12)\n",
    "                    calendar_response_text = calendar_response.text\n",
    "                    print(calendar_response_text)\n",
    "                    calendar_file_path = save_dataset_to_json(calendar_response_text, buckets['calendar'], city, listing_id)\n",
    "                    print('REVIEWS JSON CREATED')\n",
    "                    feed_forward_queue(\"parsing\",\"calendar\",city,listing_id,check_in,check_out,calendar_file_path,QUEUES_URLS)\n",
    "\n",
    "                    # DELETING MESSAGE\n",
    "                    receipt_handle = msg['ReceiptHandle']\n",
    "                    logger.info(f'The message body: {msg_body}')\n",
    "                    logger.info('Deleting message from the queue...')\n",
    "                    delete_queue_message(FROM_QUEUE_URL, receipt_handle)\n",
    "                    logger.info(f'Received and deleted message(s) from {FROM_QUEUE_URL}.')\n",
    "\n",
    "            except Exception:\n",
    "                print(Exception)\n",
    "\n",
    "    return {\n",
    "        'message': f'scrape reviews code executed correctly for city {city}'\n",
    "    }\n",
    "\n",
    "def feed_forward_queue(queue,cat,city,listing_id,check_in,check_out,file_path,QUEUES_URLS):\n",
    "    MSG_ATTRIBUTES = {\n",
    "        'Title': {\n",
    "            'DataType': 'String',\n",
    "            'StringValue': 'my_scraping_project'\n",
    "        },\n",
    "        'Author': {\n",
    "            'DataType': 'String',\n",
    "            'StringValue': 'scrapingmain'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    MSG_BODY = '{' + f'\"cat\":{cat}, \"city\":\"{city}\",\"file_path\":\"{file_path}\",\"listing_id\":\"{listing_id}\",\"check_in\":\"{check_in}\",' \\\n",
    "                        f'\"check_out\":\"{check_out}\"' + \"}\"\n",
    "\n",
    "    msg = send_queue_message(QUEUES_URLS[queue], MSG_ATTRIBUTES, MSG_BODY)\n",
    "    json_msg = json.dumps(msg, indent=4)\n",
    "\n",
    "    print(json_msg)\n",
    "    logger.info(f'''\n",
    "        Message sent to the queue {QUEUES_URLS[cat]}.\n",
    "        Message attributes: \\n{json_msg}''')\n",
    "\n",
    "    return\n",
    "\n",
    "def scrape_reviews(check_in, check_out, listing_id):\n",
    "    t0 = time.time()\n",
    "    time.sleep(random.uniform(8, 10))\n",
    "    limit = 500\n",
    "\n",
    "    url = f\"https://www.airbnb.fr/api/v3/PdpReviews?operationName=PdpReviews&locale=fr&currency=EUR&variables=%7B%22request%22%3A%7B%22fieldSelector%22%3A%22for_p3%22%2C%22limit%22%3A{limit}%2C%22listingId%22%3A%22{listing_id}%22%2C%22checkinDate%22%3A%22{check_in}%22%2C%22checkoutDate%22%3A%22{check_out}%22%2C%22numberOfAdults%22%3A%221%22%2C%22numberOfChildren%22%3A%220%22%2C%22numberOfInfants%22%3A%220%22%7D%7D&extensions=%7B%22persistedQuery%22%3A%7B%22version%22%3A1%2C%22sha256Hash%22%3A%221270212fe655c436e6a746d1f75f342030d7435045e72a1da88fe6b997ef4%22%7D%7D&_cb=1n0qrfh0bng2u30caci3r04xak1w\"\n",
    "    payload = {}\n",
    "    headers = {\n",
    "        'authority': 'www.airbnb.fr',\n",
    "        'sec-ch-ua': '\"Chromium\";v=\"94\", \"Google Chrome\";v=\"94\", \";Not A Brand\";v=\"99\"',\n",
    "        'x-csrf-token': 'V4$.airbnb.fr$B6Bqvpb43Wg$-PnMm5k8TIdG82BnjGKMqFsNs6j3v_Tvb-jqc=',\n",
    "        'x-airbnb-api-key': 'd306zoyjsyarp7ifhrjxn52tv0t20',\n",
    "        'x-niobe-short-circuited': 'true',\n",
    "        'dpr': '1.25',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "        'device-memory': '8',\n",
    "        'x-airbnb-graphql-platform-client': 'minimalist-niobe',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'x-csrf-without-token': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36',\n",
    "        'viewport-width': '954',\n",
    "        'content-type': 'application/json',\n",
    "        'x-airbnb-supports-airlock-v2': 'true',\n",
    "        'ect': '4g',\n",
    "        'x-airbnb-graphql-platform': 'web',\n",
    "        'accept': '*/*',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-mode': 'cors',\n",
    "        'sec-fetch-dest': 'empty',\n",
    "        'referer': f'https://www.airbnb.fr/rooms/{listing_id}?adults=1&check_in={check_in}&check_out={check_out}&translate_ugc=false&federated_search_id=349e1f7b-2714-4689-8ca8-0bb3fad3c103&source_impression_id=p3_1633101628_7S7qaiuvebQBLExQ&guests=1',\n",
    "        'accept-language': 'fr-FR,fr;q=0.9',\n",
    "        'cookie': 'bev=16331094_N4MmNlOGZkMTI3; _abck=2B762405C224F1DBB772AF653D2~-1~YAAQl/J6XKn6/997AQAADcFwPAYBb+ThYBavzgckmmJugyCiiioy6CnD0sfwl107k41GKlb6RfsSPxdGHVtJpZozgcGm+lzIVPTZcYsBsq5s3My8loouwk4oir4HQKJuzXfW6ODf8adhQzvn2utJHZFFAPidNYgY+ExD1/chIcn1Rd/Xix4xeHK1qVPZMBtWj//HTF08ui6cWdjIsLSrW59sRNcw5+8FeJGY5FRsLPzi+sPV+Zm2Igg2ZDULhupwwkTHiX3/HuY97+4e3IhaNfJS+4WnUSWOA4eN3+2yzo33aYT0zjLBlvLrvo+Nq/jDISpSf1/hrIouh9Wua2M808SVszJnN8BnA1SG/pi9gaGzQshLFEkyf5g==~-1~-1~-1; bm_sz=F71F9A4A79D51C3F03A8E2654643E73C~YAAQl/J6XKr6/997AQAADcFwPA2REbB9IBqNgLrtXWc9NkUVFgDWfoaGcErqfTO5noctsa+kYWmghUPK7juFbKPKO4TIPBeQJNbUSYJ6fTjrzlcKCTDbP+YkLqKgwX9+WZ6MYHjXmkqL3JfteOCm9EvuRYyCYZpMjWUZqdMojEFqX6/VEeWaITJKc14MuYEK/8bCgAJuFVNfc+CIhVMyDpFV9zDlJWwghu+xNJUVT6zUxdlqXc1+8q0xOkAAzWaHQIsRPpd0L6GFrLpr/PGV4CHCLY49nEzXJFIislwhGe7C7A==~3682865~3421252; jitney_client_session_id=42dd72af-fa4e-4a8e-8a4c-4ee93ff40155; jitney_client_session_created_at=1633101597; flags=0; frmfctr=wide; OptanonConsent=0_179751%3A1%2C0_183217%3A1%2C0_183345%3A1%2C0_183219%3A1%2C0_183240%3A1%2C0_179739%3A1%2C0_179743%3A1%2C0_185813%3A1%2C0_183096%3A1%2C0_179755%3A1%2C0_183215%3A1%2C0_185808%3A1%2C0_179747%3A1%2C0_179740%3A1%2C0_179744%3A1%2C0_185814%3A1%2C0_183097%3A1%2C0_179756%3A1%2C0_183216%3A1%2C0_183344%3A1%2C0_185809%3A1%2C0_179748%3A1%2C0_179752%3A1%2C0_183241%3A1%2C0_179741%3A1%2C0_183098%3A1%2C0_179745%3A1%2C0_183346%3A1%2C0_185811%3A1%2C0_179737%3A1%2C0_179757%3A1%2C0_179749%3A1%2C0_179753%3A1%2C0_185831%3A1%2C0_183099%3A1%2C0_179738%3A1%2C0_179742%3A1%2C0_183095%3A1%2C0_183243%3A1%2C0_179754%3A1%2C0_183214%3A1%2C0_179750%3A1%2C0_200000%3A1%2C0_200002%3A1%2C0_200003%3A1%2C0_200004%3A1%2C0_200005%3A1%2C0_200006%3A1%2C0_200007%3A1%2C0_200008%3A1%2C0_200009%3A1%2C0_200010%3A1%2C0_200011%3A1%2C0_200012%3A1%2C0_200013%3A1; OptanonAlertBoxClosed=2021-10-01T15%3A20%3A19.262Z; cdn_exp_19404f5b1ede3050a=control; cdn_exp_a09e72a4e5d5e36b4=control; cdn_exp_896b6bc8a5b375dae=control; cdn_exp_d04ec1444c962ef85=control; cdn_exp_c74f5a7707daba920=control; _user_attributes=%7B%22curr%22%3A%22EUR%22%2C%22guest_exchange%22%3A0.862845%2C%22device_profiling_session_id%22%3A%221633101597--a04e93fba79d05060a300dcb%22%2C%22giftcard_profiling_session_id%22%3A%221633101597--10532e51b48de85b9d51c1e3%22%2C%22reservation_profiling_session_id%22%3A%221633101597--bde042b99edff74d4b9d4f37%22%7D; _airbed_session_id=5868b5b04b5077bdea0c015c36fbd030; cfrmfctr=MOBILE; cdn_exp_dc7964b7e759f13ab=control; previousTab=%7B%22id%22%3A%22a3f95e2a-8993-4d9e-8329-a8870366dd19%22%2C%22url%22%3A%22https%3A%2F%2Fwww.airbnb.fr%2Frooms%2F23872481%3Fadults%3D1%26check_in%3D2021-10-08%26check_out%3D2021-10-16%26translate_ugc%3Dfalse%26federated_search_id%3D349e1f7b-2714-4689-8ca8-0bb3fad3c103%26source_impression_id%3Dp3_1633101628_7S7qaiuvebQBLExQ%26guests%3D1%22%7D; _csrf_token=V4%24.airbnb.fr%24B6Bqvpb43Wg%24-PnMm5k8iyeTIdG82BnjGKMqKlpFsNs6j3v_Tvb-jqc%3D; _gcl_au=1.1.223490689.1633101785; _uetsid=1fbc5c4022cb11eca0a2edc7f5cca087; _uetvid=1fbc88d022cb11ec96db051c744fe358; jitney_client_session_updated_at=1633101783'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f'scraping took {t1 - t0} seconds')\n",
    "\n",
    "    return response\n",
    "\n",
    "def scrape_calendar(check_in_date,check_out_date, listing_id, count=12):\n",
    "    date=check_in_date.split('-')\n",
    "    year = int(date[0])\n",
    "    month = int(date[1])\n",
    "\n",
    "    time.sleep(random.uniform(7, 10))\n",
    "\n",
    "    url = f\"https://www.airbnb.fr/api/v3/PdpAvailabilityCalendar?operationName=PdpAvailabilityCalendar&locale=fr&currency=EUR&variables=%7B%22request%22%3A%7B%22count%22%3A{count}%2C%22listingId%22%3A%22{listing_id}%22%2C%22month%22%3A{month}%2C%22year%22%3A{year}%7D%7D&extensions=%7B%22persistedQuery%22%3A%7B%22version%22%3A1%2C%22sha256Hash%22%3A%228f08e03c7bd16fcad3c92a3592c19a8b559a0d0855a84028d1163d4733ed9ade%22%7D%7D&_cb=1x8yyom0i8o7080soyczz0fb0mhb\"\n",
    "\n",
    "    payload = {}\n",
    "    headers = {\n",
    "        'authority': 'www.airbnb.fr',\n",
    "        'sec-ch-ua': '\"Chromium\";v=\"94\", \"Google Chrome\";v=\"94\", \";Not A Brand\";v=\"99\"',\n",
    "        'x-csrf-token': 'V4$.airbnb.fr$XMi705x7GDI$NxAv8KN4mVwsa9gMOCU8R9nJrugydYqmn_oL935U8kU=',\n",
    "        'x-airbnb-api-key': 'd306zoyjsyarp7ifhu67rjxn52tv0t20',\n",
    "        'x-niobe-short-circuited': 'true',\n",
    "        'dpr': '1.25',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "        'device-memory': '8',\n",
    "        'x-airbnb-graphql-platform-client': 'minimalist-niobe',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'x-csrf-without-token': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36',\n",
    "        'viewport-width': '955',\n",
    "        'content-type': 'application/json',\n",
    "        'x-airbnb-supports-airlock-v2': 'true',\n",
    "        'ect': '4g',\n",
    "        'x-airbnb-graphql-platform': 'web',\n",
    "        'accept': '*/*',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-mode': 'cors',\n",
    "        'sec-fetch-dest': 'empty',\n",
    "        'referer': f'https://www.airbnb.fr/rooms/{listing_id}?adults=1&check_in={check_in_date}&check_out={check_out_date}&translate_ugc=false&federated_search_id=dee49b7c-cc82-44b9-8756-5b35c8d13599&source_impression_id=p3_1633078381_jD3022MR4QYb7BST&guests=1',\n",
    "        'accept-language': 'fr-FR,fr;q=0.9',\n",
    "        'cookie': 'bev=163300972_MjYyTY3MGEwOTUz; _csrf_token=V4%24.airbnb.fr%24XMi705x7GDI%24NxAv8KN4mVw9gMOCU8R9nJrugydYqmn_oL35U8kU%3D; flags=0; OptanonConsent=0_179751%3A1%2C0_183217%3A1%2C0_183345%3A1%2C0_183219%3A1%2C0_183240%3A1%2C0_179739%3A1%2C0_179743%3A1%2C0_185813%3A1%2C0_183096%3A1%2C0_179755%3A1%2C0_183215%3A1%2C0_185808%3A1%2C0_179747%3A1%2C0_179740%3A1%2C0_179744%3A1%2C0_185814%3A1%2C0_183097%3A1%2C0_179756%3A1%2C0_183216%3A1%2C0_183344%3A1%2C0_185809%3A1%2C0_179748%3A1%2C0_179752%3A1%2C0_183241%3A1%2C0_179741%3A1%2C0_183098%3A1%2C0_179745%3A1%2C0_183346%3A1%2C0_185811%3A1%2C0_179737%3A1%2C0_179757%3A1%2C0_179749%3A1%2C0_179753%3A1%2C0_185831%3A1%2C0_183099%3A1%2C0_179738%3A1%2C0_179742%3A1%2C0_183095%3A1%2C0_183243%3A1%2C0_179754%3A1%2C0_183214%3A1%2C0_179750%3A1%2C0_200000%3A1%2C0_200002%3A1%2C0_200003%3A1%2C0_200004%3A1%2C0_200005%3A1%2C0_200006%3A1%2C0_200007%3A1%2C0_200008%3A1%2C0_200009%3A1%2C0_200010%3A1%2C0_200011%3A1%2C0_200012%3A1%2C0_200013%3A1; OptanonAlertBoxClosed=2021-09-30T11%3A23%3A00.013Z; cdn_exp_a09e72a4e5d5e36b4=control; cdn_exp_896b6bc8a5b375dae=treatment; cdn_exp_199358a809451d588=control; cdn_exp_d04ec1444c962ef85=control; cdn_exp_c74f5a7707daba920=micro_flex_show_by_default_web; cdn_exp_56e13c0943e268bd9=treatment; frmfctr=wide; cdn_exp_19404f5b1ede3050a=treatment; _abck=4D80B7C01412F6F36C814A8B2F4C3A76~-1~YAAQdLwvF9zBSvV7AQAAV0UNOwaNvaOxM1WEQMSu0leJTR3LqLswiaHJ82QU7ckaQfj6pRrCSfXVFw/9fgcu0xyYrn2hTwWswZo3pcVqF7X2cvI8Po2I1w9iF5tDDXd9ri8/5Xk+pNERBdi51caR9fQrTONMmKOfgw5knQJRiNsrS2Ks0Tres0KJPwocOqshXNbD7gHEsXzB+cBdCf3lknhfZifiwh9MvXtRf3/UUl67VfmVQ2GWiuBMFSQl0o4ktpTBpqhbpWltxym8+Lqk3LUSn3QAa6Txi2+UGq0G6aA7Wfy864v4GypegJJHnQak9Z9J/taCDOQwBSLUglXsBJw+DmnT0oOaxn1ZmojbwcPXtD9K8mltCJbur7YsXe2dmfLNxIWctxGR~-1~-1~-1; bm_sz=9B6D92AA7B2C9320D2C49BE72BD93BF5~YAAQdLwvF93BSvV7AQAAV0UNOw2PbpT+swji2CEIQSdjpTEeDxNfyYR7xrkBZza68Iso4FjS7GNQqCEAybACKQckHZjgtQnU7llTH2MAs14GLWIr9Ja9g8XN+52XjYrqhrsMNUeMteqaEHM/g8LDXRwX2I1QxMehDXtSRGXlDdVoQRQKYJxRxJ/VAg7iG8esTw64t0zUxjLW3haBQAKu6n7S59iDob+Nc0QyBPyzQpQw2zhNLf3fqq9/nWr+4RNw7TO37KjNWcyZVKZ0m7vNSow3lrijtNf5dnVl9WPJLoAUoQ==~4470583~3425590; jitney_client_session_id=fe672515-0dda-4997-87e5-ba2741661ce3; jitney_client_session_created_at=1633078300; cfrmfctr=MOBILE; cdn_exp_dc7964b7e759f13ab=treatment_hero_image_skeleton; previousTab=%7B%22id%22%3A%229dce4f12-c348-4cd8-bfa2-eefbeb6ef05b%22%2C%22url%22%3A%22https%3A%2F%2Fwww.airbnb.fr%2Frooms%2F19160570%3Fadults%3D1%26check_in%3D2021-11-01%26check_out%3D2021-11-09%26translate_ugc%3Dfalse%26federated_search_id%3Ddee49b7c-cc82-44b9-8756-5b35c8d13599%26source_impression_id%3Dp3_1633078381_jD3022MR4QYb7BST%26guests%3D1%22%7D; _user_attributes=%7B%22curr%22%3A%22EUR%22%2C%22guest_exchange%22%3A0.862845%2C%22device_profiling_session_id%22%3A%221633000975--df95ebe23fb135bf5f85b36b%22%2C%22giftcard_profiling_session_id%22%3A%221633078300--a2a20ae4a0bef59916fadba7%22%2C%22reservation_profiling_session_id%22%3A%221633078300--86a86cece50882e35476cb7d%22%7D; jitney_client_session_updated_at=1633078430; cbkp=2; _gcl_au=1.1.449127299.1633078433; _uetsid=c632845021e011eca528cde20bec2ddd; _uetvid=c632dc7021e011ec908be181b546cf4f'\n",
    "    }\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "def save_dataset_to_json(dataset, bucketname, city, listing_id):\n",
    "    \"\"\"function that saves a dataset to a JSON file - Needs a dataset, target directory and target filename in entry\"\"\"\n",
    "\n",
    "    file_name = f'{city}/RAW_JSON/{city}_reviews_listing_{listing_id}.json'\n",
    "\n",
    "    s3 = session.resource('s3', region_name=AWS_REGION)\n",
    "\n",
    "    s3object = s3.Object(bucketname, file_name)\n",
    "\n",
    "    s3object.put(\n",
    "        Body=(bytes(json.dumps(dataset).encode('UTF-8')))\n",
    "    )\n",
    "\n",
    "    return file_name\n",
    "\n",
    "def send_queue_message(queue_url, msg_attributes, msg_body):\n",
    "    \"\"\"\n",
    "    Sends a message to the specified queue.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = sqs_client.send_message(QueueUrl=queue_url,\n",
    "                                           MessageAttributes=msg_attributes,\n",
    "                                           MessageBody=msg_body,\n",
    "                                           MessageGroupId=str(random.randint(0,1000)))\n",
    "    except ClientError:\n",
    "        logger.exception(f'Could not send meessage to the - {queue_url}.')\n",
    "        raise\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "\n",
    "def receive_queue_message(queue_url):\n",
    "    \"\"\"\n",
    "    Retrieves one or more messages (up to 10), from the specified queue.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = sqs_client.receive_message(QueueUrl=queue_url)\n",
    "    except ClientError:\n",
    "        logger.exception(\n",
    "            f'Could not receive the message from the - {queue_url}.')\n",
    "        raise\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "\n",
    "def delete_queue_message(queue_url, receipt_handle):\n",
    "    \"\"\"\n",
    "    Deletes the specified message from the specified queue.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = sqs_client.delete_message(QueueUrl=queue_url,\n",
    "                                             ReceiptHandle=receipt_handle)\n",
    "    except ClientError:\n",
    "        logger.exception(\n",
    "            f'Could not delete the meessage from the - {queue_url}.')\n",
    "        raise\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "#  ----------------MAIN-----------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    handler(freq,buckets,QUEUES_URLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a1316",
   "metadata": {},
   "source": [
    "# 1.2 Docker <a class=\"anchor\" id=\"docker\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f128ba",
   "metadata": {},
   "source": [
    "### Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec27297",
   "metadata": {},
   "source": [
    "The scripts are then dockerized and stored on ECR to make it esay to stop and launch new EC2 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM python:3.9-slim\n",
    "\n",
    "ENV APP_DIR /ecs-example\n",
    "\n",
    "RUN mkdir -p ${APP_DIR}\n",
    "\n",
    "WORKDIR ${APP_DIR}\n",
    "\n",
    "ADD ./requirements.txt ${APP_DIR}\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY app.py ${APP_DIR}\n",
    "\n",
    "CMD [\"python3\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0402482b",
   "metadata": {},
   "source": [
    "### Requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fab54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiobotocore==1.4.2\n",
    "aiohttp==3.7.4.post0\n",
    "aioitertools==0.8.0\n",
    "async-timeout==3.0.1\n",
    "attrs==21.2.0\n",
    "boto3==1.17.78\n",
    "botocore==1.20.106\n",
    "certifi==2021.10.8\n",
    "chardet==4.0.0\n",
    "charset-normalizer==2.0.7\n",
    "fsspec==2021.10.1\n",
    "idna==3.3\n",
    "jmespath==0.10.0\n",
    "multidict==5.2.0\n",
    "numpy==1.21.2\n",
    "pandas==1.3.4\n",
    "python-dateutil==2.8.2\n",
    "pytz==2021.3\n",
    "requests==2.26.0\n",
    "s3fs==2021.10.1\n",
    "s3transfer==0.4.0\n",
    "six==1.16.0\n",
    "typing-extensions==3.10.0.2\n",
    "urllib3==1.26.7\n",
    "wrapt==1.13.2\n",
    "yarl==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23431228",
   "metadata": {},
   "source": [
    "### Store the dockerized script on AWS ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39091506",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws ecr get-login-password --region eu-west-3 | docker login --username AWS --password-stdin id.dkr.ecr.eu-west-3.amazonaws.com\n",
    "docker build . -f Dockerfile.txt -t main-scraper\n",
    "docker tag main-scraper:latest id.dkr.ecr.eu-west-3.amazonaws.com/main-scraper:latest\n",
    "docker push id.dkr.ecr.eu-west-3.amazonaws.com/main-scraper:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba1242",
   "metadata": {},
   "source": [
    "### EC2 User Data script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f7256",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo yum update \n",
    "sudo yum install unzip\n",
    "\n",
    "<!--  first install the aws CLI on the instance-->\n",
    "curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "unzip awscliv2.zip\n",
    "sudo ./aws/install\n",
    "\n",
    "<!-- then pull the script image -->\n",
    "aws ecr get-login-password --region eu-west-3 | docker login --username AWS --password-stdin id.dkr.ecr.eu-west-3.amazonaws.com\n",
    "docker pull id.dkr.ecr.eu-west-3.amazonaws.com/main-scraper:latest\n",
    "docker image ls\n",
    "\n",
    "<!-- then run a nordvpn image -->\n",
    "docker run -ti --cap-add=NET_ADMIN --name vpn -e USER=user -e PASS=mYaFAcCount -e TECHNOLOGY=NordLynx -d ghcr.io/bubuntux/nordvpn\n",
    "\n",
    "<!-- finally run the script behind the VPN -->\n",
    "docker run -it --net=container:vpn image_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b28289",
   "metadata": {},
   "source": [
    "# 2. Processing and storing in database <a class=\"anchor\" id=\"processing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf4ad6",
   "metadata": {},
   "source": [
    "## 2.1 Downloading JSONs locally for processing  <a class=\"anchor\" id=\"locally\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df80374",
   "metadata": {},
   "source": [
    "I chose to work locally because I was traveling regularly at the time and had a very poor connection in transit and needed to work locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c392e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id='access_key',\n",
    "    aws_secret_access_key='secret_access_key'\n",
    ")\n",
    "\n",
    "AWS_REGION = 'eu-west-3'\n",
    "\n",
    "s3 = session.resource('s3', region_name=AWS_REGION)\n",
    "\n",
    "# my_bucket_name=\"abnb-reviews\"\n",
    "\n",
    "\n",
    "my_bucket = s3.Bucket(my_bucket_name)\n",
    "\n",
    "prefixes=['Reunion']\n",
    "\n",
    "for prefixe in prefixes:\n",
    "    for my_bucket_object in my_bucket.objects.filter(Prefix=f'{prefixe}/RAW_JSON/'):\n",
    "        key=my_bucket_object.key\n",
    "        file_name=key.split('/')[2]\n",
    "        destination_file_path=f'C:/Users/Desktop/RUN_data/airbnb_reviews_{file_name}'\n",
    "        s3.Object(my_bucket_name, key).download_file(destination_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d1c59",
   "metadata": {},
   "source": [
    "## 2.2 Parsing and processing script <a class=\"anchor\" id=\"parsing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952ab4e",
   "metadata": {},
   "source": [
    "At this stage the main features are extracted from the JSON files for each listing. The main Feature are the main pieces of information we spontaneously seek when look for a rental on Abnb: \n",
    "   - number of bedrooms, bathrooms, beds\n",
    "   - listing type: entire home, private room\n",
    "   - location (lat, lng)... \n",
    "   \n",
    "Also at this stage an intersting feature to engineer is to group listing in small geographical areas. **Here I chose to use French areas for statistics IRIS.** \n",
    "\n",
    "Finally those features are stored in a MySQL database.\n",
    "\n",
    "Below is the script used for the main features. We use similar scripts for prices, reviews, calendar data, amenities and ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e62340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pymysql as mysql\n",
    "from pymysql import Error\n",
    "import numpy as np\n",
    "from create_database import create_database\n",
    "import geopandas\n",
    "from shapely.geometry import Point\n",
    "import pyproj\n",
    "\n",
    "def create_main_features_dataset(directory_to_parse):\n",
    "    \"\"\"function that parses all the  json files scraped from api/v3/ExploreSearch?operationName=ExploreSearch for a given city\n",
    "    and stored in a dedicated directory - Needs a directory in entry - returns a dict of unique listings and its main\n",
    "    features \"\"\"\n",
    "\n",
    "    # create 2 empty lists. The dataset list will contain a dict per listing.\n",
    "    unique_ids = []\n",
    "    dataset = []\n",
    "\n",
    "    # get all the files save in the directory to clean\n",
    "    file_list = [f for f in listdir(directory_to_parse) if isfile(join(directory_to_parse, f))]\n",
    "\n",
    "    # Loop through all the JSONs and retrieve unique ids\n",
    "    for file_name in file_list:\n",
    "        with open(f\"{directory_to_parse}/{file_name}\", 'r', encoding='utf8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                data = json.loads(data)\n",
    "            except:\n",
    "                print(file_name)\n",
    "            try:\n",
    "                items = data[\"data\"][\"dora\"][\"exploreV3\"][\"sections\"][0]['items']\n",
    "            except:\n",
    "                items = data['data']['dora'][\"exploreV3\"]['searchFooter']['sections'][0]['items']\n",
    "            for item in items:\n",
    "                pprint.pprint(item)\n",
    "                listing_id = item['listing']['id']\n",
    "                homeDetails = item['listing']['homeDetails']\n",
    "                num_chambres = 1\n",
    "                num_lits = 1\n",
    "                num_sdb = 0.5\n",
    "                for detail in homeDetails:\n",
    "                    title = detail['title']\n",
    "                    if 'chambre' in title:\n",
    "                        num_chambres = int(title.split(\" \")[0])\n",
    "                    if 'bain' in title:\n",
    "                        try:\n",
    "                            num_sdb = int(title[0])\n",
    "                        except:\n",
    "                            pass\n",
    "                    if 'lit' in title:\n",
    "                        num_lits = int(title.split(\" \")[0])\n",
    "\n",
    "                # Make sure the id is not already in the list to get only unique ids\n",
    "                if listing_id in unique_ids:\n",
    "                    pass\n",
    "                else:\n",
    "\n",
    "                    # Ad id to list of ids\n",
    "                    unique_ids.append(listing_id)\n",
    "\n",
    "                    # create a dict with all relevant/valuable features\n",
    "                    try:\n",
    "                        ':' in item['listing']['overview'][0]['title']\n",
    "                        roomTypeCategory2 = item['listing']['overview'][0]['title'].split(':')[0]\n",
    "                        housingType = item['listing']['overview'][0]['title'].split(':')[1]\n",
    "                    except:\n",
    "                        roomTypeCategory2 = item['listing']['overview'][0]['title']\n",
    "                        housingType = \"\"\n",
    "#  Data engineering: identification of iris in which the listing is located for vizualisation on the map \n",
    "                    lat = item['listing']['lat']\n",
    "                    lng = item['listing']['lng']\n",
    "\n",
    "                    # import iris data (data from https://www.geoportail.gouv.fr/)\n",
    "                    iris_gdf = geopandas.read_file('./CONTOURS-IRIS_2-1_SHP_RGR92UTM40S_REU-2021/CONTOURS-IRIS.shp')\n",
    "\n",
    "                    # projecting coordinated in the right time zone (CRS)\n",
    "                    srid = pyproj.CRS('EPSG:25832')\n",
    "                    old_srid = pyproj.CRS('EPSG:25832')\n",
    "                    transformer = pyproj.Transformer.from_crs(\"EPSG:4326\", \"EPSG:2975\", always_xy=True)\n",
    "                    x, y = transformer.transform(lng, lat)\n",
    "                    p2 = Point(x, y)\n",
    "\n",
    "                    # Association with the iris it belongs to: for each listing checking if the coordinates are contained in the iris geometry\n",
    "                    for iris_index, iris_row in iris_gdf.iterrows():\n",
    "                        if p2.within(iris_row['geometry']):\n",
    "                            iris = iris_row['NOM_IRIS']\n",
    "\n",
    "                    listing_features = {'listing_id': listing_id,\n",
    "                                        'avgRating': item['listing']['avgRating'],\n",
    "                                        'lat': lat,\n",
    "                                        'lng': lng,\n",
    "                                        'iris': iris,\n",
    "                                        'is_SuperHost': item['listing']['isSuperhost'],\n",
    "                                        'name': item['listing']['name'],\n",
    "                                        'roomTypeCategory2': roomTypeCategory2,\n",
    "                                        'housingType': housingType,\n",
    "                                        'personCapacity': item['listing']['personCapacity'],\n",
    "                                        'reviewsCount': item['listing']['reviewsCount'],\n",
    "                                        'roomTypeCategory': item['listing']['roomTypeCategory'],\n",
    "                                        'previewAmenityNames': item['listing']['previewAmenityNames'],\n",
    "                                        'num_chambres': num_chambres,\n",
    "                                        'num_lits':num_lits,\n",
    "                                        'num_sdb': num_sdb\n",
    "                                        }\n",
    "\n",
    "                    dataset.append(listing_features)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def save_dataset_to_json(dataset, target_directory, target_file_name):\n",
    "    \"\"\"function that saves a dataset to a JSON file - Needs a dataset, target directory and target filename in entry\"\"\"\n",
    "\n",
    "    with open(f'{target_directory}/{target_file_name}.json', 'w') as json_file:\n",
    "        json_file.write(json.dumps(dataset))\n",
    "\n",
    "    print(f\"JSON {target_file_name} created in directory : {target_directory}\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def save_dataset_to_mysql_db(db_name, source_directory, file_name, mysql_pw='pw', mysql_host='host'):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # IMPORTING DATA\n",
    "    dataset = pd.read_json(f\"{source_directory}/{file_name}.json\")\n",
    "\n",
    "    # CHANGING DATA TYPES     \n",
    "    dataset['avgRating'] = dataset['avgRating'].astype(\"float\")\n",
    "    dataset['lat'] = dataset['lat'].astype(\"float\")\n",
    "    dataset['lng'] = dataset['lng'].astype(\"float\")\n",
    "    dataset['is_SuperHost'] = dataset['is_SuperHost'].astype(\"string\")\n",
    "    dataset['name'] = dataset['name'].astype(\"string\")\n",
    "    dataset['roomTypeCategory2'] = dataset['roomTypeCategory2'].astype(\"string\")\n",
    "    dataset['housingType'] = dataset['housingType'].astype(\"string\")\n",
    "    dataset['roomTypeCategory'] = dataset['roomTypeCategory'].astype(\"string\")\n",
    "    dataset['previewAmenityNames'] = dataset['previewAmenityNames'].astype(\"string\")\n",
    "    dataset = dataset.replace({np.nan: None})\n",
    "    my_password = mysql_pw\n",
    "    my_host = mysql_host\n",
    "\n",
    "    #  Checking if the data exists\n",
    "    try:\n",
    "        create_database(my_password, my_host, db_name)\n",
    "    except Error as e:\n",
    "        print(\"Error while connecting to mysql:\", e)\n",
    "\n",
    "    #  connecting to the database and inserting the data\n",
    "    try:\n",
    "        connection = mysql.connect(user='root', password=my_password, database=db_name, host=my_host)\n",
    "        if connection:\n",
    "            cursor = connection.cursor()\n",
    "            table_name = \"listings_details\"\n",
    "            cursor.execute('drop table if exists calendar')\n",
    "            cursor.execute('drop table if exists ratings')\n",
    "            cursor.execute('drop table if exists prices')\n",
    "            cursor.execute('drop table if exists reviews')\n",
    "            delete_table_if_exists = f'drop table if exists {table_name}'\n",
    "            cursor.execute(delete_table_if_exists)\n",
    "            print(f'creating table {table_name}')\n",
    "            create_ads_table = f'create table {table_name} (listing_id INT PRIMARY KEY,' \\\n",
    "                               f'avgRating DECIMAL(12 , 2),' \\\n",
    "                               f'lat DECIMAL(12 , 2),' \\\n",
    "                               f'lng DECIMAL(12 , 2),' \\\n",
    "                               f'iris VARCHAR(50),' \\\n",
    "                               f'is_SuperHost VARCHAR(50),' \\\n",
    "                               f'name TEXT,' \\\n",
    "                               f'roomTypeCategory2 TEXT,' \\\n",
    "                               f'housingType TEXT,' \\\n",
    "                               f'personCapacity INT,' \\\n",
    "                               f'reviewsCount INT,' \\\n",
    "                               f'roomTypeCategory VARCHAR(50),' \\\n",
    "                               f'previewAmenity TEXT,' \\\n",
    "                               f'num_chambres INT,' \\\n",
    "                               f'num_lits INT,' \\\n",
    "                               f'num_sdb FLOAT);'\n",
    "            cursor.execute(create_ads_table)\n",
    "            print(f\"{table_name} table is created\")\n",
    "            insert_data = f\"insert into {table_name} values (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "\n",
    "            for i, row in dataset.iterrows():\n",
    "                cursor.execute(insert_data, tuple(row))\n",
    "                print(f\"record {i} inserted\")\n",
    "                connection.commit()\n",
    "    except Error as e:\n",
    "        print(\"Error while connecting to mysql:\", e)\n",
    "\n",
    "    duration = int(time.time() - t0)\n",
    "    print(f\"process completed in {duration} seconds\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Setting directory\n",
    "    source_directory = \"C:/Users/Desktop/RUN_portfolio/RUN-airbnb-main\"\n",
    "\n",
    "    #  Creating features\n",
    "    dataset = create_main_features_dataset(source_directory)\n",
    "    pprint.pprint(test_dataset)\n",
    "    \n",
    "    # Saving to JSON\n",
    "    target_directory =  \"C:/Users/Desktop/RUN_portfolio/Parsed-Data/RUN-airbnb-main\"\n",
    "    name = \"parsed_json\"\n",
    "    save_dataset_to_json(dataset, target_directory, name)\n",
    "\n",
    "    # Saving to database\n",
    "    JSON_directory=target_directory\n",
    "    db_name = \"run_airbnb\"\n",
    "    save_dataset_to_mysql_db(db_name, JSON_directory, test_json_file, mysql_pw='pw',\n",
    "                             mysql_host='host')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf67b13",
   "metadata": {},
   "source": [
    "### Final Database diagram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0971ffc8",
   "metadata": {},
   "source": [
    "Not all scripts were shown here, but here is the final tables once all the data has been parsed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5267a829",
   "metadata": {},
   "source": [
    "![{550x650}](database.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46338452",
   "metadata": {},
   "source": [
    "# Final comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c41f75d",
   "metadata": {},
   "source": [
    "At this point the data is ready for exploratory analysis !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
